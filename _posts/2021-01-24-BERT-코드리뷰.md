---
layout: post
category: MODEL
---

# BERT fine-tuning 코드 리뷰

## BERT Fine-Tuning이란?
   BERT는 대용량의 텍스트 코퍼스의 단어 임베딩을 MLM과 NSP 방식을 통해 사전학습한 transformer 모델입니다. MLM과 NSP task를 잘 수행하도록 학습된 BERT는 언어의 맥락적, 문법적 특징을 이해할 수 있다고 여겨집니다. 이와 같이 사전학습된 모델을 다시 해결하고자 하는 타겟 task를 잘 수행하도록 모델의 파라미터를 재조정하는 과정이 fine-tuning 과정입니다.<br/><br/> 이 글에서는 multi-class text classification을 잘하도록 BERT 파라미터를 fine-tuning하는 코드를 리뷰하겠습니다.
<br/>

## BERT Fine-tuning의 main함수 구성

### 1. Configuration 초기화
1) BERT 자체 configuration
```
bert_cfg = train.Config.from_json(train_cfg)
```
BERT를 사용하기 위해서는 기존에 사전학습에 사용된 하이퍼파라미터들이 필요합니다. 임베딩 차원(dim), 모델 구성 layer의 수(n_layers), input 데이터의 최대 길이(max_len) 등의 configuration을 정의해줍니다.

```
#bert_cfg 구성
{
        "dim": 768,
	"dim_ff": 3072,
	"n_layers": 12,
	"p_drop_attn": 0.1,
	"n_heads": 12,
	"p_drop_hidden": 0.1,
	"max_len": 512,
	"n_segments": 2,
	"vocab_size": 30522
}
```

2) Fine-Tuning 과정에 필요한 configuration

```
model_cfg = models.Config.from_json(model_cfg)
```
BERT fine-tuning 과정은 모델을 타겟 task로 재학습시키는 과정입니다. 학습을 위해서는 seed값, bacth 크기(batch_size), 학습률(lr) 등의 하이퍼파라미터를 정의가 필요한데, 이는 model_cfg로 초기화해줍니다.

```
#model_cfg 구성
{
    "seed": 92,
    "batch_size": 32,
    "lr": 5e-5,
    "n_epochs": 3,
    "warmup": 0.1,
    "save_steps": 500,
    "total_steps": 6000
}
```

And here is some `inline code`!

### 2. DataLoader 만들기
1) Tokenizer 정의

2) Tokenization 과정

3) DataLoader 정의


### 3. Model 정의

### 4. Train 과정

### 5. Eval 과정
